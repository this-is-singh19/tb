{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkmjfWQymRSnirNcUakbcL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/this-is-singh19/tb/blob/master/retinanet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ni9q1-Zdto7p"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import EfficientNetB0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kab1ftvtyuo",
        "outputId": "89e356f9-9623-4ce8-893f-a8b8148b6bac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "dataset_path = '/content/drive/My Drive/Dataset/imgs'\n",
        "os.chdir(dataset_path)"
      ],
      "metadata": {
        "id": "cY8BWKpHt4Sn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dirlist = ['../imgs/health/', '../imgs/sick/', '../imgs/tb']\n",
        "classes = ['Healthy', 'Sick', 'Tuberculosis']\n",
        "filepaths = []\n",
        "labels = []\n",
        "for d, c in zip(dirlist, classes):\n",
        "    flist = os.listdir(d)\n",
        "    for f in flist:\n",
        "        fpath = os.path.join(d, f)\n",
        "        filepaths.append(fpath)\n",
        "        labels.append(c)\n",
        "print ('filepaths: ', len(filepaths), '   labels: ', len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QrQ8ksqt7RI",
        "outputId": "aa66f0c5-e07e-439e-9025-56b3e00465d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filepaths:  8408    labels:  8408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Fseries = pd.Series(filepaths, name='file_paths')\n",
        "Lseries = pd.Series(labels, name='labels')\n",
        "\n",
        "# Ensure lengths match\n",
        "min_length = min(len(Fseries), len(Lseries))\n",
        "Fseries = Fseries[:min_length]\n",
        "Lseries = Lseries[:min_length]\n",
        "\n",
        "# Create the DataFrame with named columns\n",
        "df = pd.concat([Fseries, Lseries], axis=1)\n",
        "df.columns = ['file_paths', 'labels']\n",
        "\n",
        "# Count occurrences of each label\n",
        "label_counts = df['labels'].value_counts()\n",
        "print(label_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PCVipVxt8nQ",
        "outputId": "e20b024b-8900-4ea1-a406-436157a9ce02"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Healthy         3814\n",
            "Sick            3809\n",
            "Tuberculosis     785\n",
            "Name: labels, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_count = 1500\n",
        "samples = []\n",
        "\n",
        "for category in df['labels'].unique():\n",
        "    category_slice = df.query(\"labels == @category\")\n",
        "\n",
        "    if len(category_slice) < file_count:\n",
        "        # If the number of files in the category is less than file_count,\n",
        "        # sample with replacement to fill up the required number of samples\n",
        "        samples.append(category_slice.sample(file_count, replace=True, random_state=1))\n",
        "    else:\n",
        "        samples.append(category_slice.sample(file_count, replace=False, random_state=1))\n",
        "\n",
        "df = pd.concat(samples, axis=0).sample(frac=1.0, random_state=1).reset_index(drop=True)\n",
        "print(df['labels'].value_counts())\n",
        "print(len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myYq8HKFuE1p",
        "outputId": "d1d19d1c-8dbe-424c-a7bc-b06075e33623"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sick            1500\n",
            "Healthy         1500\n",
            "Tuberculosis    1500\n",
            "Name: labels, dtype: int64\n",
            "4500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(df, train_size=0.7, valid_size=0.15, test_size=0.15, random_state=None):\n",
        "    \"\"\"\n",
        "    Split the data into training, validation, and test sets.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame containing the data to be split.\n",
        "    - train_size: The proportion of data to include in the training set (default: 0.7).\n",
        "    - valid_size: The proportion of data to include in the validation set (default: 0.15).\n",
        "    - test_size: The proportion of data to include in the test set (default: 0.15).\n",
        "    - random_state: Seed for random number generation (optional).\n",
        "\n",
        "    Returns:\n",
        "    - train_df: DataFrame for training.\n",
        "    - valid_df: DataFrame for validation.\n",
        "    - test_df: DataFrame for testing.\n",
        "    \"\"\"\n",
        "    if train_size + valid_size + test_size != 1.0:\n",
        "        raise ValueError(\"The sum of train_size, valid_size, and test_size should be 1.0\")\n",
        "\n",
        "    # Split the data into training and test sets\n",
        "    train_and_valid_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    # Further split the training and validation data\n",
        "    train_df, valid_df = train_test_split(train_and_valid_df,\n",
        "                                          train_size=train_size / (train_size + valid_size),\n",
        "                                          random_state=random_state)\n",
        "\n",
        "    return train_df, valid_df, test_df\n",
        "\n",
        "def print_label_counts(df, set_name):\n",
        "    \"\"\"\n",
        "    Print label counts for a given DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame for which label counts should be printed.\n",
        "    - set_name: Name of the data set (e.g., \"Training\", \"Validation\", \"Test\").\n",
        "    \"\"\"\n",
        "    print(f\"{set_name} Set Label Counts:\")\n",
        "    label_counts = df['labels'].value_counts()\n",
        "    print(label_counts)\n",
        "\n",
        "# Split the data into train, validation, and test sets\n",
        "train_df, valid_df, test_df = split_data(df, train_size=0.7, valid_size=0.15, test_size=0.15, random_state=0)\n",
        "\n",
        "# Print label counts for each set\n",
        "print_label_counts(train_df, \"Training\")\n",
        "print_label_counts(valid_df, \"Validation\")\n",
        "print_label_counts(test_df, \"Test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AocfVjJ4uFcL",
        "outputId": "82fd085b-cd60-4199-c3e3-5c3a5108280a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Label Counts:\n",
            "Sick            1066\n",
            "Healthy         1044\n",
            "Tuberculosis    1040\n",
            "Name: labels, dtype: int64\n",
            "Validation Set Label Counts:\n",
            "Healthy         233\n",
            "Tuberculosis    229\n",
            "Sick            213\n",
            "Name: labels, dtype: int64\n",
            "Test Set Label Counts:\n",
            "Tuberculosis    231\n",
            "Healthy         223\n",
            "Sick            221\n",
            "Name: labels, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-retinanet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pSQ-lqtuyJJ",
        "outputId": "e3850be8-40da-4b55-c044-72a41520f848"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-retinanet\n",
            "  Downloading keras-retinanet-1.0.0.tar.gz (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-resnet==0.2.0 (from keras-retinanet)\n",
            "  Downloading keras-resnet-0.2.0.tar.gz (9.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from keras-retinanet) (1.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from keras-retinanet) (1.11.4)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from keras-retinanet) (3.0.8)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from keras-retinanet) (9.4.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from keras-retinanet) (4.8.0.76)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.10/dist-packages (from keras-retinanet) (4.2.0)\n",
            "Requirement already satisfied: keras>=2.2.4 in /usr/local/lib/python3.10/dist-packages (from keras-resnet==0.2.0->keras-retinanet) (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python->keras-retinanet) (1.23.5)\n",
            "Requirement already satisfied: python-utils>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from progressbar2->keras-retinanet) (3.8.2)\n",
            "Requirement already satisfied: typing-extensions>3.10.0.2 in /usr/local/lib/python3.10/dist-packages (from python-utils>=3.0.0->progressbar2->keras-retinanet) (4.5.0)\n",
            "Building wheels for collected packages: keras-retinanet, keras-resnet\n",
            "  Building wheel for keras-retinanet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-retinanet: filename=keras_retinanet-1.0.0-cp310-cp310-linux_x86_64.whl size=203822 sha256=5f7273d32218b68bcd11818df6ccabc048e3ac8325495edb81d683589c82073b\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/ee/d4/b54905a92241002b36db880e88b902ebcb015ce5ae311a16da\n",
            "  Building wheel for keras-resnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-resnet: filename=keras_resnet-0.2.0-py2.py3-none-any.whl size=20457 sha256=fb69ac2793ccdde83c4c89bf8ace95731203e3de1f5ff14a215a6da3bc7bb48a\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/af/88/a668b279c5eadbe55dcaf6207f09059135166cefb09088bacc\n",
            "Successfully built keras-retinanet keras-resnet\n",
            "Installing collected packages: keras-resnet, keras-retinanet\n",
            "Successfully installed keras-resnet-0.2.0 keras-retinanet-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras_retinanet.models import convert_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# Define the backbone (ResNet50)\n",
        "backbone = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "\n",
        "# Assuming you have a DataFrame 'train_df', 'valid_df', and 'test_df' with columns 'file_paths' and 'labels'\n",
        "# Modify the paths and column names based on your actual data structure\n",
        "\n",
        "# Data generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=True, vertical_flip=True, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, zoom_range=0.2)\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Train generator\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    train_df,\n",
        "    x_col='file_paths',\n",
        "    y_col='labels',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Validation generator\n",
        "valid_generator = valid_datagen.flow_from_dataframe(\n",
        "    valid_df,\n",
        "    x_col='file_paths',\n",
        "    y_col='labels',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Test generator\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    test_df,\n",
        "    x_col='file_paths',\n",
        "    y_col='labels',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Create the classification model using RetinaNet architecture\n",
        "classification_model = models.Sequential()\n",
        "classification_model.add(backbone)\n",
        "classification_model.add(layers.GlobalAveragePooling2D())\n",
        "classification_model.add(layers.Dense(1024, activation='relu'))\n",
        "classification_model.add(layers.Dropout(0.5))\n",
        "classification_model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "classification_model.compile(optimizer=Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "classification_model.summary()\n",
        "\n",
        "# Train the model\n",
        "checkpoint = ModelCheckpoint('retina_net_resnet50_classification.h5', save_best_only=True)\n",
        "classification_model.fit(train_generator, epochs=10, validation_data=valid_generator, callbacks=[checkpoint])\n",
        "\n",
        "# Evaluate the model\n",
        "classification_model.evaluate(test_generator)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GxJ2inRuKUi",
        "outputId": "a5c17bdf-2b08-4d37-a527-e097eb5d8893"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 0s 0us/step\n",
            "Found 3150 validated image filenames belonging to 3 classes.\n",
            "Found 675 validated image filenames belonging to 3 classes.\n",
            "Found 675 validated image filenames belonging to 3 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 2048)             0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              2098176   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 3075      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25,688,963\n",
            "Trainable params: 25,635,843\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "99/99 [==============================] - 2881s 29s/step - loss: 0.8738 - accuracy: 0.7181 - val_loss: 2.1930 - val_accuracy: 0.3156\n",
            "Epoch 2/10\n",
            "99/99 [==============================] - 2771s 28s/step - loss: 0.3741 - accuracy: 0.8537 - val_loss: 2.9104 - val_accuracy: 0.3393\n",
            "Epoch 3/10\n",
            "99/99 [==============================] - 2762s 28s/step - loss: 0.3257 - accuracy: 0.8790 - val_loss: 3.2768 - val_accuracy: 0.3452\n",
            "Epoch 4/10\n",
            "99/99 [==============================] - 2772s 28s/step - loss: 0.2834 - accuracy: 0.8889 - val_loss: 1.5631 - val_accuracy: 0.3393\n",
            "Epoch 5/10\n",
            "99/99 [==============================] - 2773s 28s/step - loss: 0.2517 - accuracy: 0.9022 - val_loss: 1.1571 - val_accuracy: 0.2844\n",
            "Epoch 6/10\n",
            "99/99 [==============================] - 2762s 28s/step - loss: 0.2765 - accuracy: 0.8965 - val_loss: 1.3238 - val_accuracy: 0.3156\n",
            "Epoch 7/10\n",
            "99/99 [==============================] - 2776s 28s/step - loss: 0.2287 - accuracy: 0.9146 - val_loss: 4.3242 - val_accuracy: 0.2356\n",
            "Epoch 8/10\n",
            "99/99 [==============================] - 2784s 28s/step - loss: 0.2260 - accuracy: 0.9200 - val_loss: 5.7832 - val_accuracy: 0.4104\n",
            "Epoch 9/10\n",
            "99/99 [==============================] - 2821s 29s/step - loss: 0.1876 - accuracy: 0.9337 - val_loss: 14.0800 - val_accuracy: 0.3407\n",
            "Epoch 10/10\n",
            "99/99 [==============================] - 2807s 28s/step - loss: 0.2680 - accuracy: 0.9025 - val_loss: 26.5135 - val_accuracy: 0.3156\n",
            "22/22 [==============================] - 173s 8s/step - loss: 25.6968 - accuracy: 0.3274\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[25.696802139282227, 0.3274074196815491]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score, recall_score\n",
        "\n",
        "\n",
        "y_pred_prob = classification_model.predict(test_generator)\n",
        "\n",
        "# Calculate accuracy\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate AUC\n",
        "auc = roc_auc_score(y_true, y_pred_prob, multi_class='ovr', average='weighted')\n",
        "print(\"AUC:\", auc)\n",
        "\n",
        "# Calculate average precision\n",
        "average_precision = average_precision_score(y_true, y_pred_prob, average='weighted')\n",
        "print(\"Average Precision:\", average_precision)\n",
        "\n",
        "# Calculate average recall\n",
        "average_recall = recall_score(y_true, y_pred, average='weighted')\n",
        "print(\"Average Recall:\", average_recall)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "5GEadynA-L4Z",
        "outputId": "fd73812c-1d9b-4f90-c86f-ed7fe3df02b2"
      },
      "execution_count": 19,
      "outputs": [
       {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 33s 33s/step\n",
            "Accuracy: 0.90625\n",
            "AUC: 0.9708333333333333\n",
            "Ave. Prec.: 0.9508805506414882\n",
            "Ave. Rec.: 0.888821548821548\n"
          ]
        }
      ]
    }
  ]
}
